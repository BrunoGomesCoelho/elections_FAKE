{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "DATA_FOLDER = \"processed_zonas_eleitorais/\"\n",
    "NEW_DATA_FOLDER = \"zonas_lat_long_per_state/\"\n",
    "NEW_FILE = \"zonas_lat_long.csv\"\n",
    "\n",
    "#ERROR_FILES = [\"empty_first.csv\", \"empty_second.csv\", \"empty_third.csv\"]\n",
    "#MORE_LOCATIONS = [\"more_than_one_first.csv\", \"more_than_one_second.csv\", \\\n",
    "#                  \"more_than_one_third.csv\"]\n",
    "\n",
    "ERROR_FILES = [\"empty_second.csv\", \"empty_third.csv\"]\n",
    "DUPLICATE_COORS_FILES = [\"more_than_one_second.csv\", \\\n",
    "                  \"more_than_one_third.csv\"]\n",
    "TOTAL_DESCONSIDERED = 0\n",
    "\n",
    "all_data = {}\n",
    "\n",
    "# Reads our files, mapping them to a dictionary for each state's name/initials\n",
    "for csv_file in pathlib.Path(DATA_FOLDER).iterdir():\n",
    "    if not csv_file.is_file():\n",
    "        continue\n",
    "    name = str(csv_file).split(\"/\")[-1]\n",
    "    all_data[name] = pd.read_csv(csv_file, encoding=\"latin1\", index_col=\"numero_zona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_files(file_list, read=True):\n",
    "    \"\"\"\n",
    "    Process a list of files, apending them into a single dataframe.\n",
    "    If read = False, a dictionary is expected\n",
    "    \"\"\"\n",
    "    data = None\n",
    "    for file in file_list:\n",
    "        if data is None:\n",
    "            data = pd.read_csv(file, encoding=\"latin1\")\n",
    "            data.columns = [\"numero_zona\"] + list(data.columns[1:])\n",
    "        else:\n",
    "            temp = pd.read_csv(file, encoding=\"latin1\")\n",
    "            temp.columns = [\"numero_zona\"] + list(temp.columns[1:])\n",
    "            data = data.append(temp, sort=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks the total amount of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 2 errors in state 'MG'\n",
      "There were 2 errors in state 'PB'\n",
      "There were 1 errors in state 'PR'\n",
      "There were 2 errors in state 'RS'\n",
      "\n",
      "In total there were 7 errors\n"
     ]
    }
   ],
   "source": [
    "# First read the wrong data\n",
    "error_data = None\n",
    "\n",
    "\"\"\"\n",
    "for error_file in ERROR_FILES:\n",
    "    if error_data is None:\n",
    "        error_data = pd.read_csv(error_file, encoding=\"latin1\")\n",
    "        error_data.columns = [\"numero_zona\"] + list(error_data.columns[1:])\n",
    "    else:\n",
    "        temp = pd.read_csv(error_file, encoding=\"latin1\")\n",
    "        temp.columns = [\"numero_zona\"] + list(temp.columns[1:])\n",
    "        error_data = error_data.append(temp, sort=False)\n",
    "\"\"\"\n",
    "error_data = read_list_files(ERROR_FILES)\n",
    "\n",
    "total_count = 0\n",
    "for state, count in error_data.groupby(\"sigla_uf.1\").size().iteritems():\n",
    "    total_count += count\n",
    "    print(f\"There were {count} errors in state '{state}'\")\n",
    "print(f\"\\nIn total there were {total_count} errors\")\n",
    "\n",
    "TOTAL_DESCONSIDERED += total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks if the multiple address were always the same lat/long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 19 duplicates in state 'MG'\n",
      "There were 6 duplicates in state 'MT'\n",
      "There were 20 duplicates in state 'PA'\n",
      "There were 11 duplicates in state 'PB'\n",
      "There were 13 duplicates in state 'PE'\n",
      "There were 7 duplicates in state 'PI'\n",
      "There were 30 duplicates in state 'PR'\n",
      "There were 20 duplicates in state 'RJ'\n",
      "There were 9 duplicates in state 'RN'\n",
      "There were 1 duplicates in state 'RR'\n",
      "There were 8 duplicates in state 'RS'\n",
      "There were 1 duplicates in state 'SC'\n",
      "There were 7 duplicates in state 'SE'\n",
      "There were 19 duplicates in state 'SP'\n",
      "There were 6 duplicates in state 'TO'\n",
      "\n",
      "In total there were 177 duplicates\n"
     ]
    }
   ],
   "source": [
    "# First read the dupliccate data\n",
    "duplicates_data = read_list_files(DUPLICATE_COORS_FILES)\n",
    "\n",
    "total_count = 0\n",
    "for state, count in duplicates_data.groupby(\"sigla_uf.1\").size().iteritems():\n",
    "    total_count += count\n",
    "    print(f\"There were {count} duplicates in state '{state}'\")\n",
    "print(f\"\\nIn total there were {total_count} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MG in practice had 0 duplicates\n",
      "MT in practice had 0 duplicates\n",
      "PA in practice had 0 duplicates\n",
      "PB in practice had 0 duplicates\n",
      "PE in practice had 0 duplicates\n",
      "PI in practice had 0 duplicates\n",
      "PR in practice had 0 duplicates\n",
      "RJ in practice had 0 duplicates\n",
      "RN in practice had 0 duplicates\n",
      "RR in practice had 0 duplicates\n",
      "RS in practice had 0 duplicates\n",
      "SC in practice had 0 duplicates\n",
      "SE in practice had 0 duplicates\n",
      "SP in practice had 0 duplicates\n",
      "TO in practice had 0 duplicates\n",
      "\n",
      "\n",
      "\n",
      "In total we will desconsider 0 zonas because of duplicate address\n",
      "\n",
      "\n",
      "--> Considering BOTH duplicate and errors, we are desconsidering 7 zonas\n"
     ]
    }
   ],
   "source": [
    "total_duplicates = 0\n",
    "duplicates = defaultdict(list)\n",
    "\n",
    "# Don't consider same lat/long as duplicates\n",
    "for state, state_data in duplicates_data.groupby(\"sigla_uf.1\"):\n",
    "    obj = all_data[state.lower()]\n",
    "    extra_cols = (len(obj.columns) - (list(obj.columns).index(\"lng_0\")+1)) // 2\n",
    "    \n",
    "    duplicate_state_count = 0\n",
    "    for num_zona in state_data[\"numero_zona\"]:\n",
    "        obj = all_data[state.lower()].query(\"numero_zona == @num_zona\").iloc[0]\n",
    "        lat, lng = obj[\"lat_0\"], obj[\"lng_0\"]\n",
    "        \n",
    "        # For each extra lat/long coordinate, check if it is the same value\n",
    "        for extra_col in range(extra_cols):\n",
    "            extra_lat, extra_lng = obj[f\"lat_{extra_col+1}\"], obj[f\"lng_{extra_col+1}\"]\n",
    "            if math.isnan(extra_lat) or math.isnan(extra_lng):\n",
    "                continue\n",
    "            else:\n",
    "                if extra_lat != lat or extra_lng != lng:\n",
    "                    print(f\"Different! {extra_lat}, {extra_lng}, {lat}, {lng}\")\n",
    "                    duplicate_state_count += 1\n",
    "                    duplicates[state.lower()].append(num_zona)\n",
    "                    total_duplicates  += 1\n",
    "                    break\n",
    "    print(f\"{state} in practice had {duplicate_state_count} duplicates\")\n",
    "print(\"\\n\\n\")\n",
    "print(f\"In total we will desconsider {total_duplicates} zonas because of duplicate address\\n\\n\")\n",
    "TOTAL_DESCONSIDERED += total_duplicates\n",
    "\n",
    "print(f\"--> Considering BOTH duplicate and errors, \"\\\n",
    "        f\"we are desconsidering {TOTAL_DESCONSIDERED} zonas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove real duplicates:\n",
    "for state in duplicates.keys():\n",
    "    for num_zon in duplicates[state]:\n",
    "        all_data[state] = all_data[state][all_data[state][\"numero_zona\"] != num_zona]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our data, both as individual files and as a single big file\n",
    "temp = None\n",
    "for state in all_data.keys():\n",
    "    all_data[state].to_csv(f\"{NEW_DATA_FOLDER}{state}.csv\", header=True, \\\n",
    "                           index=True, columns=[\"lat_0\", \"lng_0\"])\n",
    "    if temp is None:\n",
    "        temp = all_data[state]\n",
    "    else:\n",
    "        temp = temp.append(all_data[state], sort=False)\n",
    "        \n",
    "temp.to_csv(f\"{NEW_FILE}\", header=True, \\\n",
    "                       index=True, columns=[\"sigla_uf.1\", \"lat_0\", \"lng_0\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
